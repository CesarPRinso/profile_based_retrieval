{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqRSs4Hima6z"
      },
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from textblob import Word"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data preparation"
      ],
      "metadata": {
        "id": "Bud9KavgdMyS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_string(string):\n",
        "\n",
        "    string = re.sub(r\"\\'s\", \"\", string)\n",
        "    string = re.sub(r\"\\'ve\", \"\", string)\n",
        "    string = re.sub(r\"n\\'t\", \"\", string)\n",
        "    string = re.sub(r\"\\'re\", \"\", string)\n",
        "    string = re.sub(r\"\\'d\", \"\", string)\n",
        "    string = re.sub(r\"\\'ll\", \"\", string)\n",
        "    string = re.sub(r\",\", \"\", string)\n",
        "    string = re.sub(r\"!\", \" ! \", string)\n",
        "    string = re.sub(r\"\\(\", \"\", string)\n",
        "    string = re.sub(r\"\\)\", \"\", string)\n",
        "    string = re.sub(r\"\\?\", \"\", string)\n",
        "    string = re.sub(r\"'\", \"\", string)\n",
        "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
        "    string = re.sub(r\"[0-9]\\w+|[0-9]\", \"\", string)\n",
        "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
        "\n",
        "    return string.strip().lower()\n",
        "\n",
        "\n",
        "def prepare_dataset(dataset):\n",
        "    # divide in features and labels\n",
        "    x = dataset['news'].tolist()\n",
        "    y = dataset['type'].tolist()\n",
        "    print(\"\\n--------------------------------------------------------\")\n",
        "    print(\"------------------- DATA PREPARATION -------------------\")\n",
        "    print(\"--------------------------------------------------------\\n\")\n",
        "    print(\"Tokenization & lemmatization\", end='', flush=True)\n",
        "    for i, value in enumerate(x):\n",
        "        x[i] = ' '.join([Word(word).lemmatize() for word in clean_string(value).split()])\n",
        "        if(i%100==0):\n",
        "            print('.', end='', flush=True)\n",
        "    print(\"DONE!\\n\")\n",
        "    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=42)\n",
        "    vect = TfidfVectorizer(stop_words='english', min_df=2)\n",
        "\n",
        "    X_train = vect.fit_transform(X_train)\n",
        "    y_train = np.array(y_train)\n",
        "    X_test = vect.transform(X_test)\n",
        "    y_test = np.array(y_test)\n",
        "    X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.50, random_state=42)\n",
        "\n",
        "    print(\"Train set:\" + str(X_train.shape))\n",
        "    print(\"Validation set:\" + str(X_val.shape))\n",
        "    print(\"Test set:\" + str(X_test.shape))\n",
        "\n",
        "    return X_train, X_val, X_test, y_train, y_val, y_test, vect"
      ],
      "metadata": {
        "id": "g5hzRqY_c7a2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}